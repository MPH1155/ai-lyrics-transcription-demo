# ğŸµ Lyrics Transcription & Vocal Separation - AI-Powered Song Lyrics Extraction

[![Live Demo](https://img.shields.io/badge/ğŸ¤—-Live_Demo-yellow)](https://huggingface.co/spaces/MPH1155/ai-lyrics-transcription-demo)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/MPH1155/ai-lyrics-transcription-demo)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

> **Fine-tuned OpenAI Whisper model with automatic vocal separation for accurate song lyrics transcription. Compare base vs. fine-tuned performance in real-time.**

ğŸŒ **[View Static Demo Page](https://mph1155.github.io/ai-lyrics-transcription-demo/)** | ğŸš€ **[Try Live Demo on HF Spaces](https://huggingface.co/spaces/MPH1155/ai-lyrics-transcription-demo)**

![Demo Preview](docs/screenshots/results.jpg)

## âœ¨ Features

- ğŸ§  **Fine-Tuned Whisper Model**: Custom-trained on 100+ song lyrics for superior music transcription accuracy
- ğŸ¤ **Automatic Vocal Separation**: Isolates vocals using audio-separator before transcription
- ğŸ“Š **Side-by-Side Comparison**: View base Whisper vs. fine-tuned model outputs (segmented + full)
- ğŸ¨ **Modern Web UI**: Clean Bootstrap 5 interface with audio playback
- âš¡ **FastAPI Backend**: Efficient async processing with CORS support
- ğŸ³ **Docker Ready**: Deploy anywhere with included Dockerfile

## ğŸ¯ Quick Start

### Prerequisites
- Python 3.10+
- FFmpeg (for audio processing)
- 4GB+ RAM (8GB recommended)

### Local Setup
```bash
# Clone repository
git clone https://github.com/MPH1155/ai-lyrics-transcription-demo.git
cd ai-lyrics-transcription-demo

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Run the server
python -m uvicorn app:app --host 127.0.0.1 --port 8000

# Open http://127.0.0.1:8000 in your browser
```

## ğŸ“¸ Screenshots

<table>
  <tr>
    <td><img src="docs/screenshots/upload.jpg" alt="Upload Interface" width="400"/></td>
    <td><img src="docs/screenshots/results.jpg" alt="Results Page" width="400"/></td>
  </tr>
</table>

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    A[User Upload] --> B[Vocal Separation]
    B --> C[Whisper Base]
    B --> D[Whisper Fine-tuned]
    C --> E[Results Display]
    D --> E
    B --> F[Audio Playback]
    F --> E
```

## ğŸ“Š Training & Evaluation Metrics

### Training Progress
Comprehensive training metrics including loss curves, learning rate schedules, and performance graphs are available in the [`graphs/`](graphs/) directory.

<img src="docs/screenshots/training_graphs.png" alt="Training Metrics Dashboard" width="800"/>

**Key Results:**
- **Base Whisper WER**: [145.58]%
- **Fine-Tuned WER**: [89.07]%
- **Improvement**: [56.51]%
- **Training Dataset**: 100+ song lyrics

For detailed metrics and interactive SVG graphs, see [graphs/README.md](graphs/README.md).

---

## ğŸ“ Project Structure

```
ai-lyrics-transcription-demo/
â”œâ”€â”€ app.py                      # Main FastAPI application
â”œâ”€â”€ vocal_seperator.py          # Vocal separation logic
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # HF Space deployment
â”œâ”€â”€ space.yml                   # HF Space configuration
â”œâ”€â”€ src/
â”‚   â””â”€â”€ lyrics_asr/             # Core transcription package
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ infer.py            # Inference functions
â”œâ”€â”€ scripts/                    # Training & evaluation scripts
â”‚   â”œâ”€â”€ training.py             # Fine-tuning script
â”‚   â”œâ”€â”€ eval_with_HGFdata.py    # WER evaluation (public dataset)
â”‚   â”œâ”€â”€ eval_with_full_song_data.py  # WER eval (custom dataset)
â”‚   â”œâ”€â”€ create_dataset.py       # Dataset creation
â”‚   â”œâ”€â”€ upload_dataset.py       # Upload to HF Hub
â”‚   â””â”€â”€ song_downloader.py      # YouTube audio downloader
â”œâ”€â”€ graphs/                     # Training/eval visualizations
â”‚   â”œâ”€â”€ train/                  # Training metrics (SVGs)
â”‚   â”œâ”€â”€ eval/                   # Evaluation metrics (SVGs)
â”‚   â””â”€â”€ README.md               # Metrics documentation
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ index.html              # GitHub Pages showcase
â”‚   â””â”€â”€ screenshots/            # UI screenshots
â””â”€â”€ tests/                      # Unit tests
    â”œâ”€â”€ test_api.py
    â””â”€â”€ test_infer.py
```

---

## ğŸ”§ Advanced Usage

### Training the Model
```bash
cd scripts
python training.py
```
Outputs:
- Fine-tuned model: `./models/whisper-small-finetuned/`
- Training logs: `./logs/`
- Metrics graphs: `./graphs/train/`

### Evaluating Performance
```bash
# Evaluate on public dataset
python scripts/eval_with_HGFdata.py

# Evaluate on custom dataset
python scripts/eval_with_full_song_data.py
```

Results are saved as SVG graphs in `./graphs/eval/`.

### Environment Variables
Create a `.env` file (see `.env.example`):
```bash
FINETUNED_MODEL_ID=MPH1155/whisper-fine-tuned
HF_TOKEN=your_token_here  # Optional, for private models
```

---

## 5. Hugging Face Authentication
Do NOT hardcode tokens. Export environment variables instead:
```bash
# PowerShell
$Env:HUGGINGFACE_TOKEN = "hf_xxx"
```
Then inside Python:
```python
from huggingface_hub import login
import os
login(token=os.getenv("HUGGINGFACE_TOKEN"))
```

## 6. Training
```bash
python training.py
```
Outputs:
- Fine-tuned model directory: `./whisper-finetuned/`
- Logs in `./logs/`
- (Optional) Pushed model to Hub (configure push_to_hub)

## 7. Evaluation (WER)
Public dataset example:
```bash
python eval_with_HGFdata.py
```
Custom combined dataset example:
```bash
python eval_with_full_song_data.py
```

## 8. FastAPI Demo (Local)
```bash
uvicorn web_app:app --reload --port 8000
```
Open: http://localhost:8000

## 9. Deployment Options
### Option A: Hugging Face Spaces (Recommended for ML demo)
- Framework: Gradio or FastAPI Space
- Pros: GPU availability, simple model loading.
- Action: Wrap logic into a `app.py` Gradio interface or keep FastAPI and define `space_runtime.txt`.

### Option B: Vercel + Inference Backend
Vercel cannot run heavy GPU inference directly. Strategy:
1. Host inference API elsewhere (Render, HF Inference Endpoints, Replicate, or a lightweight EC2/GCP instance).
2. Create a separate `frontend/` (Next.js) deployed to Vercel.
3. Frontend calls backend REST endpoints: `/separate`, `/transcribe/base`, `/transcribe/fine`.
4. Optionally stream transcription progress via Server-Sent Events or websockets.

### Option C: All-in-one on a GPU VM
Use Docker + Nginx + systemd. Overkill for a portfolio demo unless you want DevOps credit.

## 10. Suggested Refactor (Target Structure)
```
.
â”œâ”€ src/lyrics_asr/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ data.py              # dataset prep / HF loading
â”‚  â”œâ”€ train.py             # train entry (function)
â”‚  â”œâ”€ eval.py              # compute WER
â”‚  â”œâ”€ infer.py             # chunked transcription utilities
â”‚  â”œâ”€ separate.py          # vocal separation wrapper
â”‚  â”œâ”€ cli.py               # unified CLI (train/eval/infer)
â”‚  â””â”€ config.py            # load YAML config
â”œâ”€ configs/
â”‚  â”œâ”€ train.yaml
â”‚  â”œâ”€ eval.yaml
â”‚  â””â”€ infer.yaml
â”œâ”€ scripts/
â”‚  â”œâ”€ download_dataset.ps1
â”‚  â””â”€ prepare_data.ps1
â”œâ”€ tests/
â”‚  â”œâ”€ test_infer.py
â”‚  â””â”€ fixtures/
```
Planned improvements: remove duplicated logic, centralize device selection, use argparse or Typer.

## 11. Security & Secrets
- Remove `huggingface_api.py` (currently contains a token!)
- Use `.env` with `python-dotenv` OR environment variables
- Never commit model weights if license forbids or they are large.

## 12. Roadmap
- [ ] Migrate scripts into `src/lyrics_asr`
- [ ] Introduce config-driven pipeline
- [ ] Add tests + CI workflow (GitHub Actions)
- [ ] Add small sample audio + smoke test
- [ ] Publish model + dataset cards on Hugging Face Hub
- [ ] Build Next.js frontend (optional) + deploy to Vercel
- [ ] Add Gradio demo for Spaces

## 13. Evaluation Metrics Snapshot (example)
| Model | WER (eval set) |
|-------|----------------|
| Base Whisper Small | 1.4558 |
| Fine-tuned | 0.8907 |
| Improvement | 0.5651 |

## 14. Contributing (Future)
Style: black + ruff
Tests: pytest
Environment: Python 3.10+

## 15. License
Choose a license (MIT recommended for portfolio) and add a `LICENSE` file.

---
Questions / feedback welcome. This README will evolve with the refactor.
